# æ›¼å¾·æ‹‰æ•ˆåº”æ•°æ®åˆ†æè„šæœ¬
# ç»Ÿè®¡å®ä½“é¢‘ç‡ + åˆ†ç±»éšæ—¶é—´å˜åŒ–è¶‹åŠ¿

import json
from pathlib import Path
from collections import Counter, defaultdict
from datetime import datetime
import sys

# å¯é€‰ï¼šmatplotlibç»‘å®šå¯è§†åŒ–
try:
    import matplotlib.pyplot as plt
    import matplotlib.dates as mdates
    from matplotlib import font_manager
    HAS_MATPLOTLIB = True
    
    # è®¾ç½®ä¸­æ–‡å­—ä½“
    plt.rcParams['font.sans-serif'] = ['SimHei', 'Microsoft YaHei', 'DejaVu Sans']
    plt.rcParams['axes.unicode_minus'] = False
except ImportError:
    HAS_MATPLOTLIB = False
    print("æç¤º: å®‰è£… matplotlib å¯ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨")


def load_analyzed_data(data_dir: Path):
    """åŠ è½½æ‰€æœ‰LLMåˆ†æç»“æœ"""
    all_comments = []
    
    json_files = sorted(data_dir.glob('*.json'))
    json_files = [f for f in json_files if not f.name.startswith('_')]
    
    for filepath in json_files:
        with open(filepath, 'r', encoding='utf-8') as f:
            comments = json.load(f)
            all_comments.extend(comments)
    
    return all_comments


def analyze_entities(comments):
    """ç»Ÿè®¡å®ä½“å‡ºç°é¢‘ç‡"""
    entity_counter = Counter()
    
    for comment in comments:
        analysis = comment.get('analysis', {})
        entities = analysis.get('entities', [])
        
        for entity in entities:
            # æ ‡å‡†åŒ–å¤„ç†
            entity = entity.strip().lower()
            if entity:
                entity_counter[entity] += 1
    
    return entity_counter


def analyze_categories_by_time(comments):
    """åˆ†æåˆ†ç±»éšæ—¶é—´çš„å˜åŒ–"""
    # æŒ‰å°æ—¶ç»Ÿè®¡
    hourly_stats = defaultdict(lambda: {
        'MANDELA_EFFECT': 0,
        'REBUTTAL': 0,
        'CONTENT': 0,
        'NOISE': 0
    })
    
    # æŒ‰å¤©ç»Ÿè®¡
    daily_stats = defaultdict(lambda: {
        'MANDELA_EFFECT': 0,
        'REBUTTAL': 0,
        'CONTENT': 0,
        'NOISE': 0
    })
    
    for comment in comments:
        timestamp_str = comment.get('timestamp', '')
        analysis = comment.get('analysis', {})
        category = analysis.get('category', 'NOISE')
        
        if not timestamp_str:
            continue
        
        try:
            # è§£ææ—¶é—´æˆ³
            timestamp = datetime.fromisoformat(timestamp_str.replace('Z', '+00:00'))
            
            # æŒ‰å°æ—¶
            hour_key = timestamp.strftime('%Y-%m-%d %H:00')
            hourly_stats[hour_key][category] += 1
            
            # æŒ‰å¤©
            day_key = timestamp.strftime('%Y-%m-%d')
            daily_stats[day_key][category] += 1
            
        except (ValueError, AttributeError):
            continue
    
    return dict(hourly_stats), dict(daily_stats)


def analyze_stance_distribution(comments):
    """åˆ†æç«‹åœºåˆ†å¸ƒ"""
    stance_counter = Counter()
    
    for comment in comments:
        analysis = comment.get('analysis', {})
        stance = analysis.get('stance', 'NEUTRAL')
        stance_counter[stance] += 1
    
    return stance_counter


def analyze_depth_distribution(comments):
    """åˆ†æè¯„è®ºæ·±åº¦åˆ†å¸ƒ"""
    depth_category = defaultdict(lambda: Counter())
    
    for comment in comments:
        depth = comment.get('depth', 0)
        analysis = comment.get('analysis', {})
        category = analysis.get('category', 'NOISE')
        
        depth_category[depth][category] += 1
    
    return dict(depth_category)


def print_report(comments, entity_counter, hourly_stats, daily_stats, 
                 stance_counter, depth_category):
    """æ‰“å°åˆ†ææŠ¥å‘Š"""
    print("=" * 70)
    print("æ›¼å¾·æ‹‰æ•ˆåº”è¯„è®ºæ•°æ®åˆ†ææŠ¥å‘Š")
    print("=" * 70)
    
    # åŸºæœ¬ç»Ÿè®¡
    total = len(comments)
    category_counter = Counter()
    for comment in comments:
        analysis = comment.get('analysis', {})
        category = analysis.get('category', 'NOISE')
        category_counter[category] += 1
    
    print(f"\nğŸ“Š åŸºæœ¬ç»Ÿè®¡")
    print(f"  æ€»è¯„è®ºæ•°: {total}")
    print()
    
    # åˆ†ç±»ç»Ÿè®¡
    print("ğŸ“ˆ åˆ†ç±»åˆ†å¸ƒ:")
    for cat in ['MANDELA_EFFECT', 'REBUTTAL', 'CONTENT', 'NOISE']:
        count = category_counter.get(cat, 0)
        pct = count / max(total, 1) * 100
        bar = 'â–ˆ' * int(pct / 2)
        print(f"  {cat:18} {count:5} ({pct:5.1f}%) {bar}")
    print()
    
    # ç«‹åœºç»Ÿè®¡
    print("ğŸ¯ ç«‹åœºåˆ†å¸ƒ:")
    for stance, count in stance_counter.most_common():
        pct = count / max(total, 1) * 100
        print(f"  {stance:10} {count:5} ({pct:5.1f}%)")
    print()
    
    # å®ä½“TOP 30
    print("ğŸ·ï¸  é«˜é¢‘å®ä½“ (Top 30):")
    for i, (entity, count) in enumerate(entity_counter.most_common(30), 1):
        print(f"  {i:2}. {entity:30} {count:4}")
    print()
    
    # è¯„è®ºæ·±åº¦ä¸åˆ†ç±»å…³ç³»
    print("ğŸ“ è¯„è®ºæ·±åº¦ä¸åˆ†ç±»å…³ç³»:")
    print(f"  {'æ·±åº¦':6} {'MANDELA':12} {'REBUTTAL':12} {'CONTENT':12} {'NOISE':12}")
    for depth in sorted(depth_category.keys()):
        cats = depth_category[depth]
        m = cats.get('MANDELA_EFFECT', 0)
        r = cats.get('REBUTTAL', 0)
        c = cats.get('CONTENT', 0)
        n = cats.get('NOISE', 0)
        print(f"  {depth:6} {m:12} {r:12} {c:12} {n:12}")
    print()
    
    # æ—¶é—´è¶‹åŠ¿ (æŒ‰å¤©)
    print("ğŸ“… æ¯æ—¥åˆ†ç±»å˜åŒ–:")
    sorted_days = sorted(daily_stats.keys())
    print(f"  {'æ—¥æœŸ':12} {'MANDELA':10} {'REBUTTAL':10} {'CONTENT':10} {'NOISE':10} {'æ€»è®¡':8}")
    for day in sorted_days:
        cats = daily_stats[day]
        m = cats.get('MANDELA_EFFECT', 0)
        r = cats.get('REBUTTAL', 0)
        c = cats.get('CONTENT', 0)
        n = cats.get('NOISE', 0)
        total_day = m + r + c + n
        print(f"  {day:12} {m:10} {r:10} {c:10} {n:10} {total_day:8}")
    print()
    
    # æ›¼å¾·æ‹‰æ•ˆåº”ç›¸å…³å®ä½“åˆ†æ
    print("ğŸ” æ›¼å¾·æ‹‰æ•ˆåº”ç›¸å…³å…³é”®è¯:")
    mandela_keywords = [
        'è®°å¾—', 'è®°å¿†', 'ä»¥å‰', 'çœ‹è¿‡', 'å°è±¡', 'æ„Ÿè§‰', 'å¥½åƒ',
        'æ¯•å¯¼', 'è§†é¢‘', 'åšä¸»', 'ä¸‰å¹´å‰', 'å‡ å¹´å‰'
    ]
    for kw in mandela_keywords:
        # æŸ¥æ‰¾åŒ…å«æ­¤å…³é”®è¯çš„å®ä½“
        related = [(e, c) for e, c in entity_counter.items() if kw in e]
        if related:
            total_count = sum(c for _, c in related)
            print(f"  '{kw}': {total_count} æ¬¡ - {[e for e, _ in related[:5]]}")
    
    print("\n" + "=" * 70)


def generate_visualizations(daily_stats, entity_counter, output_dir: Path):
    """ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨"""
    if not HAS_MATPLOTLIB:
        print("è·³è¿‡å¯è§†åŒ–: matplotlib æœªå®‰è£…")
        return
    
    output_dir.mkdir(parents=True, exist_ok=True)
    
    # å›¾1: æ¯æ—¥åˆ†ç±»å˜åŒ–è¶‹åŠ¿
    fig, ax = plt.subplots(figsize=(14, 6))
    
    sorted_days = sorted(daily_stats.keys())
    dates = [datetime.strptime(d, '%Y-%m-%d') for d in sorted_days]
    
    categories = ['MANDELA_EFFECT', 'REBUTTAL', 'CONTENT', 'NOISE']
    colors = ['#e74c3c', '#3498db', '#2ecc71', '#95a5a6']
    
    for cat, color in zip(categories, colors):
        values = [daily_stats[d].get(cat, 0) for d in sorted_days]
        ax.plot(dates, values, marker='o', label=cat, color=color, linewidth=2)
    
    ax.set_xlabel('æ—¥æœŸ', fontsize=12)
    ax.set_ylabel('è¯„è®ºæ•°é‡', fontsize=12)
    ax.set_title('æ›¼å¾·æ‹‰æ•ˆåº”è¯„è®ºåˆ†ç±»éšæ—¶é—´å˜åŒ–è¶‹åŠ¿', fontsize=14, fontweight='bold')
    ax.legend(loc='upper right')
    ax.grid(True, alpha=0.3)
    ax.xaxis.set_major_formatter(mdates.DateFormatter('%m-%d'))
    plt.xticks(rotation=45)
    plt.tight_layout()
    
    chart1_path = output_dir / 'category_trend.png'
    plt.savefig(chart1_path, dpi=150)
    plt.close()
    print(f"  å·²ä¿å­˜: {chart1_path}")
    
    # å›¾2: å®ä½“è¯äº‘/æ¡å½¢å›¾ (Top 20)
    fig, ax = plt.subplots(figsize=(12, 8))
    
    top_entities = entity_counter.most_common(20)
    entities = [e for e, _ in top_entities]
    counts = [c for _, c in top_entities]
    
    bars = ax.barh(range(len(entities)), counts, color='#3498db')
    ax.set_yticks(range(len(entities)))
    ax.set_yticklabels(entities)
    ax.invert_yaxis()
    ax.set_xlabel('å‡ºç°æ¬¡æ•°', fontsize=12)
    ax.set_title('é«˜é¢‘å®ä½“ Top 20', fontsize=14, fontweight='bold')
    
    # æ·»åŠ æ•°å€¼æ ‡ç­¾
    for bar, count in zip(bars, counts):
        ax.text(bar.get_width() + 0.5, bar.get_y() + bar.get_height()/2,
                str(count), va='center', fontsize=10)
    
    plt.tight_layout()
    
    chart2_path = output_dir / 'entity_frequency.png'
    plt.savefig(chart2_path, dpi=150)
    plt.close()
    print(f"  å·²ä¿å­˜: {chart2_path}")
    
    # å›¾3: åˆ†ç±»å æ¯”é¥¼å›¾
    fig, ax = plt.subplots(figsize=(8, 8))
    
    category_totals = defaultdict(int)
    for day_stats in daily_stats.values():
        for cat, count in day_stats.items():
            category_totals[cat] += count
    
    labels = list(category_totals.keys())
    sizes = list(category_totals.values())
    
    wedges, texts, autotexts = ax.pie(
        sizes, labels=labels, autopct='%1.1f%%',
        colors=colors, startangle=90,
        explode=[0.05 if l == 'MANDELA_EFFECT' else 0 for l in labels]
    )
    ax.set_title('è¯„è®ºåˆ†ç±»å æ¯”', fontsize=14, fontweight='bold')
    
    chart3_path = output_dir / 'category_pie.png'
    plt.savefig(chart3_path, dpi=150)
    plt.close()
    print(f"  å·²ä¿å­˜: {chart3_path}")


def save_report_json(comments, entity_counter, hourly_stats, daily_stats,
                     stance_counter, depth_category, output_path: Path):
    """ä¿å­˜åˆ†ææŠ¥å‘Šä¸ºJSON"""
    # åˆ†ç±»ç»Ÿè®¡
    category_counter = Counter()
    for comment in comments:
        analysis = comment.get('analysis', {})
        category = analysis.get('category', 'NOISE')
        category_counter[category] += 1
    
    report = {
        'summary': {
            'total_comments': len(comments),
            'category_distribution': dict(category_counter),
            'stance_distribution': dict(stance_counter),
        },
        'entities': {
            'top_50': entity_counter.most_common(50),
            'total_unique': len(entity_counter),
        },
        'time_series': {
            'daily': daily_stats,
            'hourly': hourly_stats,
        },
        'depth_analysis': {
            str(k): dict(v) for k, v in depth_category.items()
        }
    }
    
    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(report, f, ensure_ascii=False, indent=2)
    
    print(f"  å·²ä¿å­˜: {output_path}")


def main():
    script_dir = Path(__file__).parent
    data_dir = script_dir.parent / 'data' / 'processed' / 'llm_analyzed'
    output_dir = script_dir.parent / 'data' / 'analysis'
    output_dir.mkdir(parents=True, exist_ok=True)
    
    print("åŠ è½½æ•°æ®...")
    comments = load_analyzed_data(data_dir)
    print(f"å·²åŠ è½½ {len(comments)} æ¡è¯„è®º")
    
    if not comments:
        print("é”™è¯¯: æ²¡æœ‰æ‰¾åˆ°åˆ†ææ•°æ®ï¼Œè¯·å…ˆè¿è¡Œ batch_analyze.py")
        return
    
    print("åˆ†æä¸­...")
    
    # å„é¡¹åˆ†æ
    entity_counter = analyze_entities(comments)
    hourly_stats, daily_stats = analyze_categories_by_time(comments)
    stance_counter = analyze_stance_distribution(comments)
    depth_category = analyze_depth_distribution(comments)
    
    # æ‰“å°æŠ¥å‘Š
    print_report(comments, entity_counter, hourly_stats, daily_stats,
                 stance_counter, depth_category)
    
    # ä¿å­˜JSONæŠ¥å‘Š
    print("\nä¿å­˜æŠ¥å‘Š...")
    save_report_json(comments, entity_counter, hourly_stats, daily_stats,
                     stance_counter, depth_category,
                     output_dir / 'mandela_effect_report.json')
    
    # ç”Ÿæˆå¯è§†åŒ–
    print("\nç”Ÿæˆå¯è§†åŒ–å›¾è¡¨...")
    generate_visualizations(daily_stats, entity_counter, output_dir)
    
    print("\nâœ… åˆ†æå®Œæˆ!")


if __name__ == '__main__':
    main()
